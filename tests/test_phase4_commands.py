#!/usr/bin/env python3
"""
Test des commandes CLI de la Phase 4 - AmÃ©liorations de Performance et ParallÃ©lisation.

Ce script teste :
- Gestion du cache intelligent
- Monitoring des performances
- Analyse Monte Carlo parallÃ©lisÃ©e
- Commandes de performance
"""

import sys
import os
from pathlib import Path

# Ajouter le rÃ©pertoire src au path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_cache_manager():
    """Test du gestionnaire de cache intelligent."""
    print("ğŸ§ª Test du gestionnaire de cache intelligent...")
    
    try:
        from lcpi.aep.core.cache_manager import get_cache_manager, clear_cache, get_cache_stats
        
        # CrÃ©er une instance du cache
        cache_manager = get_cache_manager(max_size_mb=50, max_entries=100)
        print("  âœ… Cache manager crÃ©Ã© avec succÃ¨s")
        
        # Test des opÃ©rations de base
        cache_manager.set("test_key", {"data": "test_value"}, ["dep1", "dep2"])
        print("  âœ… DonnÃ©es mises en cache")
        
        # RÃ©cupÃ©ration des donnÃ©es
        cached_data = cache_manager.get("test_key")
        if cached_data and cached_data.get("data") == "test_value":
            print("  âœ… DonnÃ©es rÃ©cupÃ©rÃ©es du cache")
        else:
            print("  âŒ Erreur lors de la rÃ©cupÃ©ration des donnÃ©es")
        
        # Statistiques du cache
        stats = cache_manager.get_stats()
        if "hit_rate" in stats:
            print("  âœ… Statistiques du cache rÃ©cupÃ©rÃ©es")
        else:
            print("  âŒ Erreur lors de la rÃ©cupÃ©ration des statistiques")
        
        # Nettoyage
        clear_cache()
        print("  âœ… Cache vidÃ© avec succÃ¨s")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test du cache: {e}")
        return False


def test_performance_monitor():
    """Test du moniteur de performance."""
    print("ğŸ§ª Test du moniteur de performance...")
    
    try:
        from lcpi.aep.utils.performance_monitor import get_performance_monitor, profile_algorithm
        
        # CrÃ©er une instance du moniteur
        monitor = get_performance_monitor()
        print("  âœ… Moniteur de performance crÃ©Ã© avec succÃ¨s")
        
        # Test du profiling
        with profile_algorithm("test_algorithm") as profiler:
            # Simulation d'un algorithme
            import time
            time.sleep(0.1)
            profiler.add_metric("test_metric", 42.0, "units", "MÃ©trique de test")
            profiler.add_iteration()
        
        print("  âœ… Profiling d'algorithme rÃ©ussi")
        
        # Statistiques
        stats = monitor.get_global_stats()
        if stats["total_algorithms"] > 0:
            print("  âœ… Statistiques de performance rÃ©cupÃ©rÃ©es")
        else:
            print("  âŒ Erreur lors de la rÃ©cupÃ©ration des statistiques")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test du moniteur: {e}")
        return False


def test_parallel_monte_carlo():
    """Test de l'analyse Monte Carlo parallÃ©lisÃ©e."""
    print("ğŸ§ª Test de l'analyse Monte Carlo parallÃ©lisÃ©e...")
    
    try:
        from lcpi.aep.optimization.parallel_monte_carlo import run_parallel_monte_carlo
        
        # DonnÃ©es de test
        base_network = {
            "nodes": {
                "N1": {"demand": 0.001, "elevation": 100},
                "N2": {"demand": 0.0008, "elevation": 95}
            },
            "pipes": {
                "P1": {"from": "N1", "to": "N2", "length": 100, "diameter": 150}
            }
        }
        
        parameter_distributions = {
            "node_demand": {
                "type": "normal",
                "mean": 0.001,
                "std": 0.0002,
                "min": 0.0001,
                "max": 0.002
            }
        }
        
        # Test avec un petit nombre de simulations
        results = run_parallel_monte_carlo(
            base_network=base_network,
            parameter_distributions=parameter_distributions,
            num_simulations=10,  # Petit nombre pour le test
            solver_name="lcpi",
            max_workers=2,
            use_cache=True
        )
        
        if "analysis" in results and "execution_time" in results:
            print("  âœ… Analyse Monte Carlo parallÃ©lisÃ©e rÃ©ussie")
            print(f"     Temps d'exÃ©cution: {results['execution_time']:.3f}s")
            print(f"     Simulations: {results['analysis']['total_simulations']}")
        else:
            print("  âŒ Erreur lors de l'analyse Monte Carlo")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test Monte Carlo: {e}")
        return False


def test_performance_commands():
    """Test des commandes de performance."""
    print("ğŸ§ª Test des commandes de performance...")
    
    try:
        from lcpi.aep.commands.performance import app as performance_app
        
        # VÃ©rifier que l'app est crÃ©Ã©e
        if hasattr(performance_app, 'commands'):
            print("  âœ… Commande performance crÃ©Ã©e avec succÃ¨s")
        else:
            print("  âŒ Erreur lors de la crÃ©ation de la commande performance")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test des commandes de performance: {e}")
        return False


def test_sensitivity_commands():
    """Test des commandes de sensibilitÃ©."""
    print("ğŸ§ª Test des commandes de sensibilitÃ©...")
    
    try:
        from lcpi.aep.commands.sensitivity import app as sensitivity_app
        
        # VÃ©rifier que l'app est crÃ©Ã©e
        if hasattr(sensitivity_app, 'commands'):
            print("  âœ… Commande sensitivity crÃ©Ã©e avec succÃ¨s")
        else:
            print("  âŒ Erreur lors de la crÃ©ation de la commande sensitivity")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test des commandes de sensibilitÃ©: {e}")
        return False


def test_main_commands():
    """Test des commandes principales."""
    print("ğŸ§ª Test des commandes principales...")
    
    try:
        from lcpi.aep.commands.main import app as main_app
        
        # VÃ©rifier que l'app est crÃ©Ã©e
        if hasattr(main_app, 'commands'):
            print("  âœ… Commandes principales crÃ©Ã©es avec succÃ¨s")
            
            # VÃ©rifier les sous-commandes
            subcommands = list(main_app.commands.keys())
            expected_subcommands = [
                'solveurs', 'data', 'project', 'network', 
                'performance', 'sensitivity', 'version', 'status', 'help', 'demo'
            ]
            
            missing_subcommands = [cmd for cmd in expected_subcommands if cmd not in subcommands]
            if not missing_subcommands:
                print("  âœ… Toutes les sous-commandes sont prÃ©sentes")
            else:
                print(f"  âš ï¸  Sous-commandes manquantes: {missing_subcommands}")
        else:
            print("  âŒ Erreur lors de la crÃ©ation des commandes principales")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Erreur lors du test des commandes principales: {e}")
        return False


def main():
    """Fonction principale de test."""
    print("ğŸš€ TEST DES COMMANDES CLI DE LA PHASE 4")
    print("=" * 50)
    
    tests = [
        ("Gestionnaire de Cache", test_cache_manager),
        ("Moniteur de Performance", test_performance_monitor),
        ("Monte Carlo ParallÃ©lisÃ©", test_parallel_monte_carlo),
        ("Commandes de Performance", test_performance_commands),
        ("Commandes de SensibilitÃ©", test_sensitivity_commands),
        ("Commandes Principales", test_main_commands)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\nğŸ” {test_name}")
        print("-" * 30)
        
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"  âŒ Erreur inattendue: {e}")
            results.append((test_name, False))
    
    # RÃ©sumÃ© des tests
    print("\n" + "=" * 50)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    print("=" * 50)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nğŸ¯ RÃ©sultat: {passed}/{total} tests rÃ©ussis")
    
    if passed == total:
        print("ğŸ‰ Tous les tests sont passÃ©s ! La Phase 4 est opÃ©rationnelle.")
        return 0
    else:
        print("âš ï¸  Certains tests ont Ã©chouÃ©. VÃ©rifiez les erreurs ci-dessus.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
