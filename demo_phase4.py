#!/usr/bin/env python3
"""
D√©monstration des nouvelles fonctionnalit√©s de la Phase 4 - Am√©liorations de Performance et Parall√©lisation.

Ce script d√©montre :
- Gestion du cache intelligent
- Monitoring des performances
- Analyse Monte Carlo parall√©lis√©e
- Nouvelles commandes CLI
"""

import sys
import time
from pathlib import Path

# Ajouter le r√©pertoire src au path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def demo_cache_manager():
    """D√©monstration du gestionnaire de cache intelligent."""
    print("üóÑÔ∏è D√âMONSTRATION DU CACHE INTELLIGENT")
    print("=" * 50)
    
    try:
        from lcpi.aep.core.cache_manager import get_cache_manager, clear_cache
        
        # Cr√©er une instance du cache
        cache = get_cache_manager(max_size_mb=100, max_entries=200)
        print("‚úÖ Cache manager cr√©√© avec succ√®s")
        
        # Test des op√©rations de base
        print("\nüìä Test des op√©rations de base...")
        
        # Mettre des donn√©es en cache
        cache.set("network_config_1", {"nodes": 4, "pipes": 4, "type": "maill√©"}, ["config"])
        cache.set("hardy_cross_result_1", {"iterations": 12, "convergence": True}, ["network_config_1"])
        cache.set("epanet_result_1", {"pressures": {"N1": 25.0, "N2": 22.0}}, ["network_config_1"])
        
        print("  ‚úÖ 3 objets mis en cache")
        
        # R√©cup√©ration des donn√©es
        network_config = cache.get("network_config_1")
        hardy_result = cache.get("hardy_cross_result_1")
        epanet_result = cache.get("epanet_result_1")
        
        print(f"  ‚úÖ Donn√©es r√©cup√©r√©es: {network_config['nodes']} n≈ìuds, {network_config['pipes']} conduites")
        print(f"  ‚úÖ Hardy-Cross: {hardy_result['iterations']} it√©rations, convergence: {hardy_result['convergence']}")
        print(f"  ‚úÖ EPANET: {len(epanet_result.get('pressions', {}))} pressions calcul√©es")
        
        # Test des d√©pendances
        print("\nüîó Test des d√©pendances...")
        cache.invalidate_by_dependency("config")
        
        # V√©rifier que les objets d√©pendants sont supprim√©s
        network_config_after = cache.get("network_config_1")
        hardy_result_after = cache.get("hardy_cross_result_1")
        
        print(f"  ‚úÖ Apr√®s invalidation des d√©pendances:")
        print(f"     - Network config: {'‚ùå Supprim√©' if network_config_after is None else '‚úÖ Conserv√©'}")
        print(f"     - Hardy-Cross: {'‚ùå Supprim√©' if hardy_result_after is None else '‚úÖ Conserv√©'}")
        
        # Statistiques du cache
        stats = cache.get_stats()
        print(f"\nüìà Statistiques du cache:")
        print(f"  - Taux de succ√®s: {stats['hit_rate']*100:.1f}%")
        print(f"  - Entr√©es actuelles: {stats['current_entries']}")
        print(f"  - Taille actuelle: {stats['current_size_mb']:.1f} MB")
        print(f"  - Hits: {stats['hits']}")
        print(f"  - Misses: {stats['misses']}")
        
        # Nettoyage
        clear_cache()
        print("\nüßπ Cache vid√© avec succ√®s")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur lors de la d√©monstration du cache: {e}")
        return False


def demo_performance_monitor():
    """D√©monstration du moniteur de performance."""
    print("\nüìä D√âMONSTRATION DU MONITEUR DE PERFORMANCE")
    print("=" * 50)
    
    try:
        from lcpi.aep.utils.performance_monitor import get_performance_monitor, profile_algorithm
        
        # Cr√©er une instance du moniteur
        monitor = get_performance_monitor()
        print("‚úÖ Moniteur de performance cr√©√© avec succ√®s")
        
        # Test du profiling
        print("\nüîç Test du profiling d'algorithmes...")
        
        # Simuler plusieurs algorithmes
        algorithms = ["hardy_cross", "epanet_simulation", "optimization_genetic"]
        
        for i, alg_name in enumerate(algorithms):
            with profile_algorithm(f"{alg_name}_test_{i}") as profiler:
                # Simulation d'un algorithme
                time.sleep(0.1)  # Simulation d'un calcul
                
                # Ajouter des m√©triques
                profiler.add_metric("iterations", i + 5, "count", "Nombre d'it√©rations")
                profiler.add_metric("convergence", 0.95 + i * 0.01, "ratio", "Taux de convergence")
                profiler.add_metric("memory_peak", 50 + i * 10, "MB", "Pic de m√©moire")
                
                # Marquer une it√©ration
                profiler.add_iteration()
                
                print(f"  ‚úÖ {alg_name}: profil√© avec succ√®s")
        
        # Statistiques globales
        stats = monitor.get_global_stats()
        print(f"\nüìà Statistiques globales:")
        print(f"  - Total d'algorithmes: {stats['total_algorithms']}")
        print(f"  - Temps d'ex√©cution total: {stats['total_execution_time']:.3f}s")
        print(f"  - Pic de m√©moire global: {stats['total_memory_peak']:.1f} MB")
        print(f"  - Ex√©cutions r√©ussies: {stats['successful_runs']}")
        
        # Statistiques par algorithme
        print(f"\nüîç Statistiques par algorithme:")
        for alg_name in algorithms:
            alg_stats = monitor.get_algorithm_stats(f"{alg_name}_test_0")
            if "error" not in alg_stats:
                print(f"  - {alg_name}: {alg_stats['total_runs']} ex√©cutions, "
                      f"taux de succ√®s: {alg_stats['success_rate']*100:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur lors de la d√©monstration du moniteur: {e}")
        return False


def demo_parallel_monte_carlo():
    """D√©monstration de l'analyse Monte Carlo parall√©lis√©e."""
    print("\nüöÄ D√âMONSTRATION MONTE CARLO PARALL√âLIS√â")
    print("=" * 50)
    
    try:
        from lcpi.aep.optimization.parallel_monte_carlo import run_parallel_monte_carlo
        
        # Donn√©es de test
        base_network = {
            "nodes": {
                "N1": {"demand": 0.001, "elevation": 100},
                "N2": {"demand": 0.0008, "elevation": 95},
                "N3": {"demand": 0.0006, "elevation": 90}
            },
            "pipes": {
                "P1": {"from": "N1", "to": "N2", "length": 100, "diameter": 150},
                "P2": {"from": "N2", "to": "N3", "length": 80, "diameter": 125}
            }
        }
        
        parameter_distributions = {
            "node_demand": {
                "type": "normal",
                "mean": 0.001,
                "std": 0.0002,
                "min": 0.0001,
                "max": 0.002
            },
            "pipe_roughness": {
                "type": "uniform",
                "min": 100,
                "max": 150
            }
        }
        
        print("üìã Configuration de l'analyse Monte Carlo:")
        print(f"  - R√©seau: {len(base_network['nodes'])} n≈ìuds, {len(base_network['pipes'])} conduites")
        print(f"  - Param√®tres variables: {len(parameter_distributions)}")
        print(f"  - Simulations: 20 (petit nombre pour la d√©mo)")
        print(f"  - Workers: 2")
        
        # Lancer l'analyse Monte Carlo parall√©lis√©e
        print("\n‚ö° Lancement de l'analyse...")
        start_time = time.time()
        
        results = run_parallel_monte_carlo(
            base_network=base_network,
            parameter_distributions=parameter_distributions,
            num_simulations=20,  # Petit nombre pour la d√©mo
            solver_name="lcpi",
            max_workers=2,
            use_cache=True
        )
        
        execution_time = time.time() - start_time
        
        # Affichage des r√©sultats
        print(f"\n‚úÖ Analyse termin√©e en {execution_time:.3f} secondes")
        
        analysis = results['analysis']
        stats = results['stats']
        
        print(f"\nüìä R√©sultats de l'analyse:")
        print(f"  - Total de simulations: {analysis['total_simulations']}")
        print(f"  - Simulations r√©ussies: {analysis['successful_simulations']}")
        print(f"  - Taux de succ√®s: {analysis['success_rate']*100:.1f}%")
        print(f"  - Taux de cache: {analysis['cache_hit_rate']*100:.1f}%")
        print(f"  - Temps moyen par simulation: {analysis['average_execution_time']:.3f}s")
        
        print(f"\n‚ö° Statistiques des workers:")
        print(f"  - Workers utilis√©s: {stats.get('max_workers', 'N/A')}")
        print(f"  - T√¢ches totales: {stats.get('total_tasks', 'N/A')}")
        print(f"  - T√¢ches compl√©t√©es: {stats.get('completed_tasks', 'N/A')}")
        print(f"  - Cache hits: {stats.get('cache_hits', 'N/A')}")
        print(f"  - Cache misses: {stats.get('cache_misses', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur lors de la d√©monstration Monte Carlo: {e}")
        return False


def demo_cli_commands():
    """D√©monstration des nouvelles commandes CLI."""
    print("\nüîß D√âMONSTRATION DES NOUVELLES COMMANDES CLI")
    print("=" * 50)
    
    try:
        # Test des commandes de performance
        from lcpi.aep.commands.performance import app as performance_app
        print("‚úÖ Commande performance cr√©√©e avec succ√®s")
        
        # Test des commandes de sensibilit√©
        from lcpi.aep.commands.sensitivity import app as sensitivity_app
        print("‚úÖ Commande sensitivity cr√©√©e avec succ√®s")
        
        # Test des commandes principales
        from lcpi.aep.commands.main import app as main_app
        print("‚úÖ Commandes principales cr√©√©es avec succ√®s")
        
        print("\nüìã Commandes disponibles:")
        print("  üöÄ Performance:")
        print("    - lcpi aep performance profile")
        print("    - lcpi aep performance monitor")
        print("    - lcpi aep performance cache")
        print("    - lcpi aep performance benchmark")
        print("    - lcpi aep performance report")
        print("    - lcpi aep performance optimize")
        
        print("  üìä Sensibilit√©:")
        print("    - lcpi aep sensitivity parallel")
        print("    - lcpi aep sensitivity distributions")
        print("    - lcpi aep sensitivity validate")
        
        print("  üåä Principales:")
        print("    - lcpi aep version")
        print("    - lcpi aep status")
        print("    - lcpi aep help")
        print("    - lcpi aep demo")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Erreur lors de la d√©monstration des commandes CLI: {e}")
        return False


def main():
    """Fonction principale de d√©monstration."""
    print("üéâ D√âMONSTRATION DES NOUVELLES FONCTIONNALIT√âS DE LA PHASE 4")
    print("üöÄ Am√©liorations de Performance et Parall√©lisation")
    print("=" * 70)
    
    demos = [
        ("Gestionnaire de Cache Intelligent", demo_cache_manager),
        ("Moniteur de Performance", demo_performance_monitor),
        ("Monte Carlo Parall√©lis√©", demo_parallel_monte_carlo),
        ("Nouvelles Commandes CLI", demo_cli_commands)
    ]
    
    results = []
    
    for demo_name, demo_func in demos:
        print(f"\n{'='*70}")
        try:
            success = demo_func()
            results.append((demo_name, success))
        except Exception as e:
            print(f"‚ùå Erreur inattendue lors de {demo_name}: {e}")
            results.append((demo_name, False))
    
    # R√©sum√© des d√©monstrations
    print(f"\n{'='*70}")
    print("üìä R√âSUM√â DES D√âMONSTRATIONS")
    print("=" * 70)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for demo_name, success in results:
        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHOU√â"
        print(f"  {status} {demo_name}")
    
    print(f"\nüéØ R√©sultat: {passed}/{total} d√©monstrations r√©ussies")
    
    if passed == total:
        print("\nüéâ Toutes les d√©monstrations sont r√©ussies !")
        print("üöÄ La Phase 4 est pleinement op√©rationnelle.")
        print("\nüí° Prochaines √©tapes:")
        print("  1. Tester les commandes CLI avec de vrais fichiers de configuration")
        print("  2. Optimiser les performances sur des r√©seaux plus complexes")
        print("  3. Documenter les nouvelles fonctionnalit√©s")
        print("  4. Pr√©parer la Phase 5 : Interface Utilisateur")
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} d√©monstration(s) √† corriger")
    
    return 0 if passed == total else 1


if __name__ == "__main__":
    sys.exit(main())
